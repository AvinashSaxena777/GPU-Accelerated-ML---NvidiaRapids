# GPU-Accelerated-ML---NvidiaRapids

## Nvidia Hackathon

### Kaggle Competition and Data Link:
https://www.kaggle.com/competitions/odsc-2024-nvidia-hackathon

### Description:
* Processed 11 million records using NVIDIA Rapids and CUDA DataFrame, implementing a GPU-accelerated regressor model with an error rate of 0.015. 
* Engineered features using GPU-supported libraries for outlier analysis, robust scaling, and text vectorization, improving model performance by 25%.
* Experimented with cumlâ€™s XGBoost, Random Forest, and SGD Regressor models, achieving best results on A100 GPU, reducing training time by 90%.


### How to run:
* Download dataset from Kaggle competition link https://www.kaggle.com/competitions/odsc-2024-nvidia-hackathon, it has 11 million records and 108 features.
* To leverage GPU. install rapids following instructions from link: https://docs.rapids.ai/install/
* Run the script on GPU environment. 
